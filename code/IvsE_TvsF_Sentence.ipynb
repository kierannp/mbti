{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow_hub as hub\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embedding_model = hub.load(module_url)\n",
    "def embed(input):\n",
    "    return embedding_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/mbti_1.csv\")\n",
    "sen_extra = []\n",
    "sen_posts = []\n",
    "sen_nature = []\n",
    "number_posts = 10\n",
    "types = pd.unique(data['type'])\n",
    "code = {tp:i for i,tp in enumerate(types)}\n",
    "rev_code  = {i:tp for i,tp in enumerate(types)}\n",
    "\n",
    "for index, row in data.iterrows(): \n",
    "    if len(row['posts'].split('|||')) < 10:\n",
    "        continue\n",
    "    embedded_posts = embed(row['posts'].split('|||')[:number_posts])\n",
    "    labels_extra = [(0 if 'E' in row['type'] else 1)] * number_posts\n",
    "    labels_nature = [(0 if 'T' in row['type'] else 1)] * number_posts\n",
    "    post = [post.numpy() for post in embedded_posts]\n",
    "    sen_extra.extend(labels_extra)\n",
    "    sen_nature.extend(labels_nature)\n",
    "    sen_posts.extend(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2436/2436 [==============================] - 5s 2ms/step - loss: 1.2109 - dense_6_loss: 0.5441 - dense_7_loss: 0.6668 - dense_6_accuracy: 0.7673 - dense_7_accuracy: 0.5935 - val_loss: 1.2002 - val_dense_6_loss: 0.5371 - val_dense_7_loss: 0.6631 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5967\n",
      "Epoch 2/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.1845 - dense_6_loss: 0.5375 - dense_7_loss: 0.6470 - dense_6_accuracy: 0.7679 - dense_7_accuracy: 0.6238 - val_loss: 1.2061 - val_dense_6_loss: 0.5417 - val_dense_7_loss: 0.6644 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5963\n",
      "Epoch 3/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.1657 - dense_6_loss: 0.5304 - dense_7_loss: 0.6354 - dense_6_accuracy: 0.7653 - dense_7_accuracy: 0.6366 - val_loss: 1.2144 - val_dense_6_loss: 0.5510 - val_dense_7_loss: 0.6634 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.6038\n",
      "Epoch 4/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.1438 - dense_6_loss: 0.5245 - dense_7_loss: 0.6193 - dense_6_accuracy: 0.7679 - dense_7_accuracy: 0.6560 - val_loss: 1.2213 - val_dense_6_loss: 0.5452 - val_dense_7_loss: 0.6762 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.6003\n",
      "Epoch 5/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.1138 - dense_6_loss: 0.5173 - dense_7_loss: 0.5965 - dense_6_accuracy: 0.7666 - dense_7_accuracy: 0.6783 - val_loss: 1.2565 - val_dense_6_loss: 0.5487 - val_dense_7_loss: 0.7079 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5967\n",
      "Epoch 6/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.0767 - dense_6_loss: 0.5079 - dense_7_loss: 0.5688 - dense_6_accuracy: 0.7696 - dense_7_accuracy: 0.6984 - val_loss: 1.2706 - val_dense_6_loss: 0.5730 - val_dense_7_loss: 0.6977 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5942\n",
      "Epoch 7/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.0495 - dense_6_loss: 0.4999 - dense_7_loss: 0.5496 - dense_6_accuracy: 0.7703 - dense_7_accuracy: 0.7149 - val_loss: 1.3457 - val_dense_6_loss: 0.5780 - val_dense_7_loss: 0.7678 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5784\n",
      "Epoch 8/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.0322 - dense_6_loss: 0.5007 - dense_7_loss: 0.5315 - dense_6_accuracy: 0.7666 - dense_7_accuracy: 0.7310 - val_loss: 1.3208 - val_dense_6_loss: 0.5698 - val_dense_7_loss: 0.7510 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5847\n",
      "Epoch 9/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 1.0043 - dense_6_loss: 0.4893 - dense_7_loss: 0.5150 - dense_6_accuracy: 0.7705 - dense_7_accuracy: 0.7421 - val_loss: 1.5076 - val_dense_6_loss: 0.6903 - val_dense_7_loss: 0.8173 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5911\n",
      "Epoch 10/10\n",
      "2436/2436 [==============================] - 4s 2ms/step - loss: 0.9916 - dense_6_loss: 0.4930 - dense_7_loss: 0.4985 - dense_6_accuracy: 0.7682 - dense_7_accuracy: 0.7536 - val_loss: 1.4061 - val_dense_6_loss: 0.6167 - val_dense_7_loss: 0.7894 - val_dense_6_accuracy: 0.7748 - val_dense_7_accuracy: 0.5772\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "\n",
    "inputs = tf.keras.Input(shape=(embedding_dim,))\n",
    "\n",
    "dense1_1 = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "drop1_1 = layers.Dropout(.2)(dense1_1)\n",
    "dense1_2 = layers.Dense(256, activation='relu')(drop1_1)\n",
    "drop1_2 = layers.Dropout(.2)(dense1_2)\n",
    "dense1_3 = layers.Dense(64, activation=\"relu\")(drop1_2)\n",
    "drop1_3 = layers.Dropout(.2)(dense1_3)\n",
    "\n",
    "dense2_1 = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "drop2_1 = layers.Dropout(.2)(dense2_1)\n",
    "dense2_2 = layers.Dense(256, activation='relu')(drop2_1)\n",
    "drop2_2 = layers.Dropout(.2)(dense2_2)\n",
    "dense2_3 = layers.Dense(64, activation=\"relu\")(drop2_2)\n",
    "drop2_3 = layers.Dropout(.2)(dense2_3)\n",
    "\n",
    "output1 = layers.Dense(1, activation='sigmoid')(drop1_3)\n",
    "output2 = layers.Dense(1, activation='sigmoid')(drop2_3)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs= [output1, output2])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss=['binary_crossentropy','binary_crossentropy'],\n",
    "    metrics=[\n",
    "        'accuracy'\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    np.stack(sen_posts),\n",
    "    [np.array(sen_extra),np.array(sen_nature)],\n",
    "    batch_size=32,\n",
    "    validation_split=.1,\n",
    "    epochs=10\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.ModelCheckpoint('./sentence_multi_out_model.h5', save_best_only=True, save_weights_only=True)\n",
    "#     ]\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"jsweights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
