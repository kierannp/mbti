{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts\n",
    "- I could embed the URLs or YouTube links into in embedding space outputting a vector\n",
    "- I can consider each sentence individually as a classsification problem, predictind a type from each sentence and then averaging all the sentence classes\n",
    "- Gonna change it so the indiviudal posts are just one long paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Embedding, Dense, Dropout\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Input\n",
    "from absl import logging\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from custom_gen import DataGenerator\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to load the universal sentence encoder which is a encoder created by Google that can encode any Word sentence or paragraph to a 512 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embedding_model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return embedding_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(data):\n",
    "    max_imgs, max_gifs, max_vids, max_words = 0,0,0,0 \n",
    "    vocab = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if len(imgs[i]) > max_imgs:\n",
    "            max_imgs = len(imgs[i])\n",
    "        if len(gifs[i]) > max_imgs:\n",
    "            max_gifs = len(gifs[i])\n",
    "        if len(vids[i]) > max_imgs:\n",
    "            max_vids = len(vids[i])\n",
    "        if len(paragraphs[i].split(' ')) > max_words:\n",
    "            max_words = len(paragraphs[i].split(' '))        \n",
    "        for word in paragraphs[i].split(' '):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return max_imgs, max_vids, max_gifs, max_words, len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and create reverse and forward map things to the MBTI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"embeddings_added.csv\")\n",
    "data = pd.read_csv('data/sentence_data.csv')\n",
    "# to_array = lambda x : np.array( [float(i) for i in x[1:-1].split(',')] )\n",
    "# data['posts'] = data['posts'].apply(to_array)\n",
    "\n",
    "\n",
    "header_data = pd.read_csv('data/mbti_1.csv')\n",
    "types = pd.unique(header_data['type'])\n",
    "code = {tp:i for i,tp in enumerate(types)}\n",
    "rev_code  = {i:tp for i,tp in enumerate(types)}\n",
    "# for tp in types:\n",
    "#     data['type'] = data['type'].replace(tp,code[tp])\n",
    "    \n",
    "### integer mapping using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(data['type'])\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "# ### One hot encoding\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Datasets\n",
    "# partition = {'train': [], 'validation': ['id-4']}\n",
    "# labels = {'id-1': 0, 'id-2': 1, 'id-3': 2, 'id-4': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we need to pull out the URLs from the user posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = {i:[] for i in range(data.shape[0])}\n",
    "vids = {i:[] for i in range(data.shape[0])}\n",
    "gifs = {i:[] for i in range(data.shape[0])}\n",
    "misc = {i:[] for i in range(data.shape[0])}\n",
    "words = {i:[] for i in range(data.shape[0])}\n",
    "paragraphs = []\n",
    "\n",
    "for i, person in enumerate(data['posts']):\n",
    "    urls_person = [re.findall(\"(?P<url>https?://[^\\s]+)\", post) for post in person.split('|||')]\n",
    "    w = [re.sub(\"(?P<url>https?://[^\\s]+)\", '', post) for post in person.split('|||')]\n",
    "    urls_clean = []\n",
    "    \n",
    "    for url, ws in zip(urls_person,w): \n",
    "        if url:\n",
    "            urls_clean.append(url[0])\n",
    "        if ws:\n",
    "            words[i].append(re.sub(' +', ' ',ws))\n",
    "    \n",
    "    for url in urls_clean:\n",
    "        if (\".jpg\" in url) or (\".png\" in url):\n",
    "            imgs[i].append(url)\n",
    "        elif \"www.youtube.com\" in url:\n",
    "            vids[i].append(url)\n",
    "        elif \".gif\" in url:\n",
    "            gifs[i].append(url)\n",
    "        else:\n",
    "            misc[i].append(url)\n",
    "    para = ''\n",
    "    for sentence in words[i]:\n",
    "        para += sentence + ' '\n",
    "    paragraphs.append(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce logging output.\n",
    "# logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# message_embeddings = embed(paragraphs)\n",
    "\n",
    "# for i, message_embedding in enumerate(np.array(message_embeddings).tolist()[0:1]):\n",
    "#   print(\"Message: {}\".format(paragraphs[i]))\n",
    "#   print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "#   message_embedding_snippet = \", \".join((str(x) for x in message_embedding[0:1]))\n",
    "#   print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "\n",
    "    \n",
    "# posts = []\n",
    "# for message in np.array(message_embeddings).tolist():\n",
    "#     posts.append(message)\n",
    "# data['USE'] = posts\n",
    "# data.to_csv(\"embeddings_added.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sentence_data = pd.DataFrame()\n",
    "# sentences = []\n",
    "# label = []\n",
    "# for w in range(data.shape[0]):\n",
    "#     for sen in embed(words[w]):\n",
    "#         sentences.append( sen.numpy().tolist() )\n",
    "#     label.extend([data['type'][w]] * len(words[w]))\n",
    "# sentence_data['posts'] = sentences\n",
    "# sentence_data['type'] = label\n",
    "# sentence_data.to_csv('sentence_data.csv')\n",
    "\n",
    "\n",
    "# for i in range(data['posts'].shape[0]):\n",
    "#     np.save('data/{}.npy'.format(i),data['posts'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = np.zeros((data.shape[0],512))\n",
    "# data['USE'][0][1:-1]\n",
    "# encoding = np.array([list(map(float,i[1:-1].split(','))) for i in data['USE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = np.stack(data['posts'].values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoding,onehot_encoded, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "# model.fit(np.expand_dims(X_train, axis=2), y_train, epochs=40, validation_data=(np.expand_dims(X_test, axis=2),y_test),steps_per_epoch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTP'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = embed([\"I eat shit for breakfast. Good thing its not vegan\"])\n",
    "rev_code[np.argmax(model.predict(test))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
